{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "139e0258-47fa-4048-93b2-012a223df987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bfa69b7-7ffa-461e-9447-20c0bace46d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2 - Developing a Simple Pipeline\n",
    "\n",
    "In this demonstration, we will create a simple Lakeflow Spark Declarative Pipeline project using the new **Lakeflow Pipeline Editor** with declarative SQL.\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Describe the SQL syntax used to create a Lakeflow Spark Declarative Pipeline.\n",
    "- Navigate the Lakeflow Pipeline Editor to modify pipeline settings and ingest the raw data source file(s).\n",
    "- Create, execute and monitor a Spark Declarative Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc57acf8-2253-4154-ab46-5b8b79c6ee88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE (your cluster starts with **labuser**)\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a99a9d7-dc7b-4bfe-aa70-1c7cf667e66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course.\n",
    "\n",
    "This cell will also reset your `/Volumes/dbacademy/ops/labuser/` volume with the JSON files to the starting point, with one JSON file in each volume.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically create and reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ba20d8-9d59-4c9a-8c0a-c9628f5e2b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5aea0d3-966c-4d34-9611-e104933e5a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Developing and Running a Spark Declarative Pipeline with the Lakeflow Pipeline Editor\n",
    "\n",
    "This course includes a simple, pre-configured Spark Declarative Pipeline to explore and modify. \n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "- Explore the Lakeflow Pipeline Editor and the declarative SQL syntax  \n",
    "- Modify pipeline settings  \n",
    "- Run the Spark Declarative Pipeline and explore the streaming tables and materialized view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4801fe81-bc08-4dfa-933b-9c13fb16644e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below and **copy the path** from the output cell to your **dbacademy.ops.labuser** volume. You will need this path when modifying your pipeline settings. \n",
    "\n",
    "   This volume path contains the **orders**, **status** and **customer** directories, which contain the raw JSON files.\n",
    "\n",
    "   **EXAMPLE PATH**: `/Volumes/dbacademy/ops/labuser1234_5678@vocareum.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a73c712-33a6-4364-a9d8-d386fff59ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(DA.paths.working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ea3f9a-3471-4502-9ebd-05c1059047af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. In this course we have starter files for you to use in your pipeline. This demonstration uses the folder **2 - Developing a Simple Pipeline Project**. To create a pipeline and add existing assets to associate it with code files already available in your Workspace (including Git folders) complete the following:\n",
    "\n",
    "   a. For ease of use, open **Jobs & Pipelines** in a separate tab:\n",
    "\n",
    "    - On the main navigation bar, right-click on **Jobs & Pipelines** and select **Open in a New Tab**.\n",
    "\n",
    "   b. In **Jobs & Pipelines** select **Create** â†’ **ETL Pipeline**.\n",
    "\n",
    "   c. Complete the pipeline creation page with the following:\n",
    "\n",
    "    - **Name**: `Name-your-pipeline-using-this-notebook-name-add-your-first-name` \n",
    "    - **Default catalog**: Select your **labuser** catalog  \n",
    "    - **Default schema**: Select your **default** schema (database)\n",
    "    - Notice there are a variety of options to start your pipeline.\n",
    "\n",
    "   d. In the options, select **Add existing assets**. In the popup, complete the following:\n",
    "\n",
    "    - **Pipeline root folder**: Select the **2 - Developing a Simple Pipeline Project** folder: \n",
    "      - `/Workspace/Users/your-lab-user-name/build-data-pipelines-with-lakeflow-spark-declarative-pipelines-en_us-3.x.x/Build Data Pipelines with Lakeflow Spark Declarative Pipelines/2 - Developing a Simple Pipeline Project`\n",
    "\n",
    "    - **Source code paths**: Within the same root folder as above, select the **orders** folder: \n",
    "      - `/Workspace/Users/your-lab-user-name/build-data-pipelines-with-lakeflow-spark-declarative-pipelines-en_us-3.x.x/Build Data Pipelines with Lakeflow Spark Declarative Pipelines/2 - Developing a Simple Pipeline Project/orders`\n",
    "\n",
    "    **NOTE:** You can select folders containing SQL and Python files to be executed as part of the pipeline, or you can provide individual file paths. The specified files will be processed when the pipeline runs.\n",
    "\n",
    "   e. Click **Add**, This will create a pipeline and associate the correct files for this demonstration.\n",
    "\n",
    "**Example**\n",
    "\n",
    "<img src=\"./Includes/images/demo02_existing_assets.png\" alt=\"Setting Pipeline Assets\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04c425bb-9fb6-43cc-8f56-49b384f424c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. In the new window, select the **orders_pipeline.sql** file and follow the instructions in the SQL file within the **Lakeflow Pipelines Editor**. \n",
    "\n",
    "    Leave this notebook open as you will use it later.\n",
    "\n",
    "![Orders File Directions](./Includes/images/demo02_select_orders_sqlfile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f227271-224e-4f2a-930b-92e453cd49f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Add a New File to Cloud Storage\n",
    "\n",
    "1. After exploring and executing the pipeline by following the instructions in the **`orders_pipeline.sql`** file, run the cell below to add a new JSON file (**01.json**) to your volume at:  `/Volumes/dbacademy/ops/labuser-your-id/orders`.\n",
    "\n",
    "   **NOTE:** If you receive the error `name 'DA' is not defined`, you will need to rerun the classroom setup script at the top of this notebook to create the `DA` object. This is required to correctly reference the path and successfully copy the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93c6d088-cbc5-44ea-8903-4b02a6a2f9b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "copy_files('/Volumes/dbacademy_retail/v01/retail-pipeline/orders/stream_json', \n",
    "           f'{DA.paths.working_dir}/orders', \n",
    "           n = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccd9d3d2-7a3d-441d-ac8a-381dfb7343ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the following steps to view the new file in your volume:\n",
    "\n",
    "   a. Select the **Catalog** icon ![Catalog Icon](./Includes/images/catalog_icon.png) from the left navigation pane.  \n",
    "\n",
    "   b. Expand your **dbacademy.ops.labuser** volume.  \n",
    "\n",
    "   c. Expand the **orders** directory. You should see two files in your volume: **00.json** and **01.json**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8167d4d9-902e-4a8a-aa1e-275053c67b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the cell below to view the data in the new **/orders/01.json** file. Notice the following:\n",
    "\n",
    "   - The **01.json** file contains new orders.  \n",
    "   - The **01.json** file has 25 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "383e32e3-1cc8-4332-bee9-d6de7e559fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.sql(f'''\n",
    "  SELECT *\n",
    "  FROM json.`{DA.paths.working_dir}/orders/01.json`\n",
    "''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54271e5a-ce82-401f-88e2-41b761a9ba46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Go back to the **orders_pipeline.sql** file and select **Run Pipeline** to execute your ETL pipeline again with the new file (Step 13).  \n",
    "\n",
    "   Watch the pipeline run and notice only 25 rows are added to the bronze and silver tables. \n",
    "\n",
    "   This happens because the pipeline has already processed the first **00.json** file (174 rows), and it is now only reading the new **01.json** file (25 rows), appending the rows to the streaming tables, and recomputing the materialized view with the latest data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d195e28-a4f6-4aae-96df-3a4e3c42bcb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Exploring Your Streaming Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7179770d-0019-441c-8239-726c5a43a646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. View the new streaming tables and materialized view in your catalog. Complete the following:\n",
    "\n",
    "   a. Select the catalog icon ![Catalog Icon](./Includes/images/catalog_icon.png) in the left navigation pane.\n",
    "\n",
    "   b. Expand your **labuser** catalog.\n",
    "\n",
    "   c. Expand the schemas **1_bronze_db**, **2_silver_db**, and **3_gold_db**. Notice that the two streaming tables and materialized view are correctly placed in your schemas.\n",
    "\n",
    "      - **labuser.1_bronze_db.orders_bronze_demo2**\n",
    "\n",
    "      - **labuser.2_silver_db.orders_silver_demo2**\n",
    "\n",
    "      - **labuser.3_gold_db.orders_by_date_gold_demo2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4d05d3f-f52b-467c-ae4e-3b4659eab84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cell below to view the data in the **labuser.1_bronze_db.orders_bronze_demo2** table. Before you run the cell, how many rows should this streaming table have?\n",
    "\n",
    "   Notice the following:\n",
    "      - The table contains 199 rows (**00.json** had 174 rows, and **01.json** had 25 rows).\n",
    "      - In the **source_file** column you can see the exact file the rows were ingested from.\n",
    "      - In the **processing_time** column you can see the exact time the rows were ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed8d8a3f-e89f-4975-8420-4d10af9ce0ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM 1_bronze_db.orders_bronze_demo2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e39346e2-3b23-4490-a052-4cdb96f48ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Complete the following steps to view the history of the **orders_bronze_demo2** streaming table:\n",
    "\n",
    "   a. Select the **Catalog** icon ![Catalog Icon](./Includes/images/catalog_icon.png) in the left navigation pane.  \n",
    "\n",
    "   b. Expand the **labuser.1_bronze_db** schema.  \n",
    "\n",
    "   c. Click the three-dot (ellipsis) icon next to the **orders_bronze_demo2** table.  \n",
    "\n",
    "   d. Select **Open in Catalog Explorer**.  \n",
    "\n",
    "   e. In the Catalog Explorer, select the **History** tab. Notice an error is returned because viewing the history of a streaming table requires **SHARED_COMPUTE**. In our labs we use a **DEDICATED (formerly single user)** cluster.\n",
    "\n",
    "   f. Above your catalogs on the left select your compute cluster and change it to the provided **shared_warehouse**.\n",
    "\n",
    "   ![Change Compute](./Includes/images/change_compute.png)  \n",
    "\n",
    "   g. Go back and look at the last two versions of the table. Notice the following:  \n",
    "\n",
    "      - In the **Operation** column, the last two updates were **STREAMING UPDATE**.  \n",
    "\n",
    "      - Expand the **Operation Parameters** values for the last two updates. Notice both use `\"outputMode\": \"Append\"`.  \n",
    "\n",
    "      - Find the **Operation Metrics** column. Expand the values for the last two updates. Observe the following:\n",
    "\n",
    "         - It displays various metrics for the streaming update: **numRemovedFiles, numOutputRows, numOutputBytes, and numAddedFiles**.  \n",
    "\n",
    "         - In the `numOutputRows` values, 174 rows were added in the first update, and 25 rows in the second.\n",
    "\n",
    "   h. Close the Catalog Explorer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f021edd-6e1c-4eaf-bd25-098d4b1db798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Viewing Spark Declarative Pipelines with the Pipelines UI\n",
    "\n",
    "After exploring and creating your pipeline using the **orders_pipeline.sql** file in the steps above, you can view the pipeline(s) you created in your workspace via the **Jobs and Pipelines** UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5b847d5-04ef-45df-9199-8b07f5466bcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Complete the following steps to view the pipeline you created:\n",
    "\n",
    "   a. In the main applications navigation pane on the far left (you may need to expand it by selecting the ![Expand Navigation Pane](./Includes/images/expand_main_navigation.png) icon at the top left of your workspace) right-click on **Jobs & Pipelines** and select **Open Link in a New Tab**.\n",
    "\n",
    "   b. This should take you to the pipelines you have created. You should see your **2 - Developing a Simple Pipeline Project - labuser** pipeline.\n",
    "\n",
    "   c. Select your **2 - Developing a Simple Pipeline Project - labuser**. Here, you can use the UI to modify the pipeline.\n",
    "\n",
    "   d. Select the **Settings** button at the top. This will take you to the settings within the UI.\n",
    "\n",
    "   e. Select **Schedule** to schedule the pipeline. Select **Cancel**, we will learn how to schedule the pipeline later.\n",
    "\n",
    "   f. Under your pipeline name, select the drop-down with the time date stamp. Here you can view the **Pipeline graph** and other metrics for each run of the pipeline.\n",
    "\n",
    "   g. Close the pipeline UI tab you opened.\n",
    "\n",
    "   ![Jobs & Pipelines](./Includes/images/demo_2_view_in_jobs_pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eadd7127-1f63-4510-8cda-ab3e30a218b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Lakeflow Spark Declarative Pipelines](https://docs.databricks.com/aws/en/dlt/) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5aa816e-a63d-41fb-9990-3f7df24c0439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "2 - Developing a Simple Pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "SQL",
   "language": "sql",
   "name": "sql"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
