{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "868e562e-f16d-4506-97b0-0316291f1241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93e0d052-5071-44e0-bdb2-b4535f4f4424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Status Pipeline\n",
    "\n",
    "In this DLT pipeline, the **status** pipeline is implemented within a notebook. DLT supports using `.py`, `.sql`, or notebook files as pipeline sources.\n",
    "\n",
    "**NOTE:** In an notebook you must use either Python or SQL. \n",
    "\n",
    "The final pipeline performs the following tasks:\n",
    "\n",
    "1. Creates the **status_bronze_demo5** streaming table by ingesting raw JSON files from `/Volumes/dbacademy/ops/your-lab-user/status/`.\n",
    "\n",
    "2. Creates the **status_silver_demo5** streaming table from the **status_bronze_demo5** table.\n",
    "\n",
    "3. Creates the materialized view **full_order_status_gold_demo5** to capture each order's status by joining the following tables:\n",
    "   - **status_silver_demo5**\n",
    "   - **orders_silver_demo5**\n",
    "\n",
    "4. Creates the following materialized views:\n",
    "   - **cancelled_orders_gold_demo5** – Displays all cancelled orders and how many days passed before cancellation.\n",
    "   - **delivered_orders_gold_demo5** – Displays all delivered orders and how many days it took to deliver each order.\n",
    "![Pipeline](../../Includes/images/demo6_pipeline_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7385c865-40fd-4f14-b8e8-8da67dc24453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. JSON -> Bronze\n",
    "The code below ingests JSON files located in your `/Volumes/dbacademy/ops/your-lab-user/status/` volume, using the `source` DLT configuration parameter to point to the base path `/Volumes/dbacademy/ops/your-lab-user/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26104d0-2223-4819-a115-968dc09513a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE 1_bronze_db.status_bronze_demo6\n",
    "  COMMENT \"Ingest raw JSON order status files from cloud storage\"\n",
    "  TBLPROPERTIES (\n",
    "    \"quality\" = \"bronze\",\n",
    "    \"pipelines.reset.allowed\" = false     -- prevent full table refreshes on the bronze table\n",
    "  )\n",
    "AS \n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() processing_time, \n",
    "  _metadata.file_name AS source_file\n",
    "FROM STREAM read_files(\n",
    "  \"${source}/status\", \n",
    "  format => \"json\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f28de351-a1ff-4e34-92b0-51cd86bc03f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Bronze -> Silver\n",
    "The code below performs a simple transformation on the date field and selects only the necessary columns for the silver streaming table **status_silver_demo5**.  \n",
    "\n",
    "We're also adding a comment and table properties to document the table for production use, along with DLT expectations to enforce data quality on the streaming table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "821d4705-17d6-495d-ada5-f863ca672eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE 2_silver_db.status_silver_demo6\n",
    "  (\n",
    "    -- Drop rows if order_status_timestamp is not valid\n",
    "    CONSTRAINT valid_timestamp EXPECT (order_status_timestamp > \"2021-12-25\") ON VIOLATION DROP ROW,\n",
    "    -- Warn if order_status is not in the following\n",
    "    CONSTRAINT valid_order_status EXPECT (order_status IN ('on the way','canceled','return canceled','delivered','return processed','placed','preparing'))\n",
    "  )\n",
    "  COMMENT \"Order with each status and timestamp\"\n",
    "  TBLPROPERTIES (\"quality\" = \"silver\")\n",
    "AS \n",
    "SELECT\n",
    "  order_id,\n",
    "  order_status,\n",
    "  timestamp(status_timestamp) AS order_status_timestamp\n",
    "FROM STREAM 1_bronze_db.status_bronze_demo6;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce93d8c7-9611-4044-9f99-7b717d03a7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Use a Materialized View to Join Two Tables\n",
    "One way to join two streaming tables in DLT is by creating a materialized view that performs the join.  This approach takes all rows from each streaming table and executes a full inner join operation.\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "- **Materialized views include built-in optimizations where applicable:**\n",
    "  - [Incremental refresh for materialized views](https://docs.databricks.com/aws/en/optimizations/incremental-refresh)\n",
    "  - [Delta Live Tables Announces New Capabilities and Performance Optimizations](https://www.databricks.com/blog/2022/06/29/delta-live-tables-announces-new-capabilities-and-performance-optimizations.html)\n",
    "  - [Cost-effective, incremental ETL with serverless compute for Delta Live Tables pipelines](https://www.databricks.com/blog/cost-effective-incremental-etl-serverless-compute-delta-live-tables-pipelines)\n",
    "\n",
    "- **Stateful joins (Stream to Stream):** For stateful joins in DLT (i.e., joining incrementally as data is ingested), refer to the [Optimize stateful processing in DLT with watermarks](https://docs.databricks.com/aws/en/dlt/stateful-processing) documentation. **Stateful joins are an advanced topic and outside the scope of this course.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3886d80c-0f02-464c-8558-77d176099993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.full_order_info_gold_demo6\n",
    "  COMMENT \"Joining the orders and order status silver tables to view all orders with each individual status per order\"\n",
    "  TBLPROPERTIES (\"quality\" = \"gold\")\n",
    "AS \n",
    "SELECT\n",
    "  orders.order_id,\n",
    "  orders.order_timestamp,\n",
    "  status.order_status,\n",
    "  status.order_status_timestamp\n",
    "-- Notice that the STREAM keyword was not used when referencing the streaming tables\n",
    "FROM 2_silver_db.status_silver_demo6 status    \n",
    "  INNER JOIN 2_silver_db.orders_silver_demo6 orders \n",
    "  ON orders.order_id = status.order_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9baf25dc-7ff4-4a3b-96a1-6464c94ed103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Create Materialized Views for Cancelled and Delivered Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ce70a6-9a4d-4361-98a6-56189eb4ee03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The code below will create two tables using the joined data from above:\n",
    "\n",
    "- **3_gold_db.cancelled_orders_gold_demo5**\n",
    "    - A materialized view containing all **cancelled** orders\n",
    "    - number of days it took to cancel each order.\n",
    "\n",
    "- **3_gold_db.delivered_orders_gold_demo5**\n",
    "    - A materialized view containing all **delivered** orders\n",
    "    - number of days it took to deliver each order.\n",
    "\n",
    "    [datediff function](https://docs.databricks.com/aws/en/sql/language-manual/functions/datediff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4022776d-afd9-47b7-bcfc-6dbfe43d8711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- CANCELLED ORDERS MV\n",
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.cancelled_orders_gold_demo6\n",
    "  COMMENT \"All cancelled orders\"\n",
    "  TBLPROPERTIES (\"quality\" = \"gold\")\n",
    "AS \n",
    "SELECT\n",
    "  order_id,\n",
    "  order_timestamp,\n",
    "  order_status,\n",
    "  order_status_timestamp,\n",
    "  datediff(DAY,order_timestamp, order_status_timestamp) AS days_to_cancel -- calculate days to cancel\n",
    "FROM 3_gold_db.full_order_info_gold_demo6\n",
    "WHERE order_status = 'canceled';\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-- DELIVERED ORDERS MV\n",
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.delivered_orders_gold_demo6\n",
    "  COMMENT \"All delivered orders\"\n",
    "  TBLPROPERTIES (\"quality\" = \"gold\")\n",
    "AS \n",
    "SELECT\n",
    "  order_id,\n",
    "  order_timestamp,\n",
    "  order_status,\n",
    "  order_status_timestamp,\n",
    "  datediff(DAY,order_timestamp, order_status_timestamp) AS days_to_delivery -- calculate days to deliver\n",
    "FROM 3_gold_db.full_order_info_gold_demo6\n",
    "WHERE order_status = 'delivered';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f71345c-3f43-4392-aca4-3034c3cc9633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Create the Production Pipeline\n",
    "Follow the steps below to modify the pipeline settings and run the production pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d98b92a-6009-4a00-b930-dbbb3a31ae9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Complete the following steps to modify your pipeline configuration for production:\n",
    "\n",
    "   a. Select the **Settings** icon ![Pipeline Settings](../../Includes/images/pipeline_settings_icon.png) in the left navigation pane.\n",
    "\n",
    "   b. In the **Pipeline settings** section, you can modify the **Pipeline name** and **Run as** settings.\n",
    "\n",
    "      - Click the pencil icon ![pencil_settings_icon.png](../../Includes/images/pencil_settings_icon.png) next to **Run as**.\n",
    "\n",
    "      - You can optionally change the executor of the pipeline to a service principal.  \n",
    "        A service principal is an identity you create in Databricks for use with automated tools, jobs, and applications.  \n",
    "\n",
    "        - For more information, see the [What is a service principal?](https://docs.databricks.com/aws/en/admin/users-groups/service-principals#what-is-a-service-principal) documentation.\n",
    "\n",
    "      - For this course select **Cancel** and leave **Run as** set to your username.\n",
    "\n",
    "   c. In the **Code assets** section, confirm that:\n",
    "\n",
    "      - **Root folder** points to this DLT project (**5 - Deploying a DLT Pipeline to Production**).\n",
    "\n",
    "      - **Source code** references the **orders** and **status** folders within this project.\n",
    "\n",
    "   d. In the **Default location for data assets** section, confirm the following:\n",
    "\n",
    "      - **Default catalog** is your **labuser** catalog.\n",
    "\n",
    "      - **Default schema** is the **default** schema.\n",
    "\n",
    "   e. In the **Compute** section, confirm that **Serverless** compute is selected.\n",
    "\n",
    "   f. In the **Configuration** section, ensure that the `source` key is set to your data source volume path:  \n",
    "      `/Volumes/dbacademy/ops/your-labuser-name`\n",
    "\n",
    "   g. In the **Tags** section you can add takes to help determine usage per department/chargeback.. We will leave them as is. \n",
    "\n",
    "   h. In the **Advanced settings** section:\n",
    "\n",
    "      - Expand **Advanced settings**.\n",
    "\n",
    "      - Click **Edit advanced settings**.\n",
    "\n",
    "      - In **Pipeline mode**, ensure **Triggered** is selected so the pipeline runs on a schedule.  \n",
    "        - Alternatively, you can choose **Continuous** mode to keep the pipeline running at all times.  \n",
    "        - For more details, see [Triggered vs. continuous pipeline mode](https://docs.databricks.com/aws/en/dlt/pipeline-mode).\n",
    "\n",
    "      - In **PIpeline user mode** select **Production**.\n",
    "\n",
    "      - For **Channel**, you can leave it as **Preview** for training purposes:\n",
    "        - **Current** – Uses the latest stable Databricks Runtime version, recommended for production.\n",
    "        - **Preview** – Uses a more recent, potentially less stable Runtime version, ideal for testing upcoming features.\n",
    "        - View the [DLT release notes and the release upgrade process](https://docs.databricks.com/aws/en/release-notes/dlt/) documentation for more information.\n",
    "\n",
    "      - In the **Event logs** section:\n",
    "        - Select **Publish event log to metastore**.\n",
    "        - Set **Event log name** to `event_log_demo_5`.\n",
    "        - Set **Event log catalog** to your **labuser** catalog.\n",
    "        - Set **Event log schema** to the **default** schema.\n",
    "\n",
    "        **NOTE:** If the event log is not saved to the correct location, the event log exploration steps will not work properly in the main notebook.\n",
    "\n",
    "   i. Click **Save** to save your DLT pipeline settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d3c1d60-ce29-442f-a262-480c7f5b2e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Once your pipeline is production-ready, you'll want to schedule it to run either on a time interval or continuously.\n",
    "\n",
    "   For this demonstration, we’ll:\n",
    "   - Schedule the pipeline to run every day at 8:00 PM.\n",
    "   - Optionally configure notifications to alert you upon job **Start**, **Success**, and **Failure**.  \n",
    "     *(If you don’t want email notifications, you can skip this step.)*\n",
    "\n",
    "   Complete the following steps to schedule the pipeline:\n",
    "\n",
    "   a. Select the **Schedule** button.\n",
    "\n",
    "   b. For the job name, leave it as **5 - Deploying a DLT Pipeline to Production Project - labuser-name**.\n",
    "\n",
    "   c. Below **Job name**, select **Advanced**.\n",
    "\n",
    "   d. In the **Schedule** section, configure the following:\n",
    "   - Set the **Day**.\n",
    "   - Set the time to **20:00** (8:00 PM).\n",
    "   - Leave the **Timezone** as default.\n",
    "   - Select **More options**, and under **Notifications**, add your email to receive alerts for:\n",
    "     - **Start**\n",
    "     - **Success**\n",
    "     - **Failure**\n",
    "\n",
    "   e. Click **Create** to save and schedule the job.\n",
    "\n",
    "  **NOTE:** You could also set the pipeline to run a few minutes after your current time to see it start through the scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9c3ed2a-db6f-4b91-95c2-108c23c1e24b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. After completing the pipeline settings and scheduling the pipeline, let's manually trigger the pipeline by selecting the **Run pipeline** button.\n",
    "\n",
    "    While the pipeline is running, you can explore what the final pipeline will look like using the image below:\n",
    "\n",
    "    ![DLT Pipeline Demo 6](../../Includes/images/demo6_pipeline_image.png) \n",
    "\n",
    "**NOTE:** Currently we have one JSON file in both **status** and **orders**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7587d88-9d1f-4064-aba0-56cb4f216151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. After the pipeline has completed it's first run, complete the following:\n",
    "\n",
    "   a. Examine the **Pipeline graph** and confirm:\n",
    "      - 174 rows were read into the **orders_bronze** and **orders_silver** streaming tables\n",
    "      - 536 rows were read into the **status_bronze** and **status_silver** streaming tables\n",
    "      - 536 rows are in the **full_order_info_gold** materialized view\n",
    "      - 7 rows are in the **orders_by_date_gold** materialized view\n",
    "      - 8 rows are in the **cancelled_orders_gold** materialized view\n",
    "      - 94 rows are in the **delivered_orders_gold** materialized view\n",
    "\n",
    "   b. Go back to the main notebook **5 - Deploying a DLT Pipeline to Production**\n",
    "\n",
    "   c. Complete the steps in step **D. Land Data Data to Your Data Source Volume**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "562baf4e-a701-41c0-b3f3-5814ff306d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. After you have landed **4** new files into the data source volume, run the pipeline to process the newly landed JSON files.\n",
    "\n",
    "   Notice the following:\n",
    "\n",
    "   a. The **status** bronze to silver flow ingests 410 new rows.\n",
    "\n",
    "   b. The **orders** bronze to silver flow ingests 98 new rows.\n",
    "\n",
    "   c. The **full_order_info_gold** materialized view join contains a total of 946 rows (the previous 536 rows + the new 410 rows).\n",
    "\n",
    "   d. The **cancelled_orders_gold** materialized view contains 21 rows.\n",
    "\n",
    "   e. The **delivered_orders_gold** materialized view contains 176 rows.\n",
    "\n",
    "   f. The **orders_by_date_gold** materialized view contains 11 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "801469ff-2db8-4d63-82dc-166ddaf96810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. In the window at the bottom, select the **Expectations** link for the **status_silver_demo5** table. It should contain the value **2**. Notice that in this run, 7.6% (31 rows) for the **valid_order_status** expectation returned a warning.\n",
    "\n",
    "This is something we would want to investigate and address in future stages of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b58a7bdc-8317-47a8-8bd9-3ebdae5152cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Go back to the main notebook **5 - Deploying a DLT Pipeline to Production** and complete the steps in step **E. Monitor Your Pipeline with the Event Log**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7398d1ac-b502-46c3-9638-019520a46160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "status_pipeline.sql",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "SQL",
   "language": "sql",
   "name": "sql"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
