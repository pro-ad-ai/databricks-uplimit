{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd64be0f-13a0-4255-8ae5-e73c74d233f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "199a785e-4e2b-4764-9214-7cf0628e126e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5 - Deploying a Pipeline to Production\n",
    "\n",
    "In this demonstration, we will begin by adding an additional data source to our pipeline and performing a join with our streaming tables. Then, we will focus on productionalizing the pipeline by adding comments and table properties to the objects we create, scheduling the pipeline, and creating an event log to monitor the pipeline.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Apply the appropriate comment syntax and table properties to pipeline objects to enhance readability.\n",
    "- Demonstrate how to perform a join between two streaming tables using a materialized view to optimize data processing.\n",
    "- Execute the scheduling of a pipeline using trigger or continuous modes to ensure timely processing.\n",
    "- Explore the event log to monitor a production Lakeflow Spark Declarative Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d5e9efd-8a79-4352-9d59-3e0bb1ab8e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE (your cluster starts with **labuser**)\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9269ad5-ecad-4985-a38c-514ced883038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course.\n",
    "\n",
    "This cell will also reset your `/Volumes/dbacademy/ops/labuser/` volume with the JSON files to the starting point, with one JSON file in each volume.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically create and reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdbc3131-4623-42b6-b238-862e624b8979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f8c15c0-8886-4d9f-b1ac-b6a40a540099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore the Orders and Status JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "585ba6f8-2d36-4231-8338-248df88ccd40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Explore the raw data located in the `/Volumes/dbacademy/ops/our-lab-user/orders/` volume. This is the data we have been working with throughout the course demonstrations.\n",
    "\n",
    "   Run the cell below to view the results. Notice that the orders JSON file(s) contains information about when each order was placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a5d4f56-f9a3-4f86-b060-ede0292695c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM read_files(\n",
    "  DA.paths_working_dir || '/orders/',\n",
    "  format => 'JSON'\n",
    ")\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c645336-6d4f-4258-89e5-0522cbcbc687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Explore the **status** raw data located in the `/Volumes/dbacademy/ops/your-lab-user/status/` volume and filter for the specific **order_id** *75123*.\n",
    "\n",
    "   Run the cell below to view the results. Notice that the status JSON file(s) contain **order_status** information for each order.  \n",
    "\n",
    "   **NOTE:** The **order_status** can include multiple rows per order and may be any of the following:\n",
    "\n",
    "   - on the way  \n",
    "   - canceled  \n",
    "   - return canceled  \n",
    "   - reported shipping error  \n",
    "   - delivered  \n",
    "   - return processed  \n",
    "   - return picked up  \n",
    "   - placed  \n",
    "   - preparing  \n",
    "   - return requested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13629908-0ae1-4332-b80e-33e62d8d779e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM read_files(\n",
    "  DA.paths_working_dir || '/status/',\n",
    "  format => 'JSON'\n",
    ")\n",
    "WHERE order_id = 75123;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c8ec0a2-eb4b-4512-a070-d5b57dc1ef6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. One of our objectives is to join the **orders** data with the order **status** data.  \n",
    "\n",
    "    The query below demonstrates what the result of the final join in the Spark Declarative Pipeline will look like after the data has been incrementally ingested and cleaned when we create the pipeline. Run the cell and review the output.\n",
    "\n",
    "    Notice that after joining the tables, we can see each **order_id** along with its original **order_timestamp** and the **order_status** at specific points in time.\n",
    "\n",
    "**NOTE:** The data used in this demo is artificially generated, so the **order_status_timestamps** may not reflect realistic timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00260be4-dfa8-4f7e-9ad0-c7c13e75b6d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WITH orders AS (\n",
    "  SELECT *\n",
    "  FROM read_files(\n",
    "        DA.paths_working_dir || '/orders/',\n",
    "        format => 'JSON'\n",
    "  )\n",
    "),\n",
    "status AS (\n",
    "  SELECT *\n",
    "  FROM read_files(\n",
    "        DA.paths_working_dir || '/status/',\n",
    "        format => 'JSON'\n",
    "  )\n",
    ")\n",
    "-- Join the views to get the order history with status\n",
    "SELECT\n",
    "  orders.order_id,\n",
    "  timestamp(orders.order_timestamp) AS order_timestamp,\n",
    "  status.order_status,\n",
    "  timestamp(status.status_timestamp) AS order_status_timestamp\n",
    "FROM orders\n",
    "  INNER JOIN status \n",
    "  ON orders.order_id = status.order_id\n",
    "ORDER BY order_id, order_status_timestamp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3693252e-c196-42fa-be1d-d0695e43277f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Putting a Pipeline in Production\n",
    "\n",
    "This course includes a complete Lakeflow Spark Declarative Pipeline project that has already been created.  In this section, you'll explore the Spark Declarative Pipeline and modify its settings for production use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "064701e9-3f29-4a6e-b13a-571882a3daf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. The screenshot below shows what the final Spark Declarative Pipeline will look like when ingesting a single JSON file from the data sources:  \n",
    "![Final Demo 6 Pipeline](./Includes/images/demo5_pipeline_image_run1.png)\n",
    "\n",
    "    **Note:** Depending on the number of files you've ingested, the row count may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1b67dfa-adcc-4db7-8149-c7393d061ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cell below to create your starter Spark Declarative Pipeline for this demonstration. The pipeline will set the following for you:\n",
    "    - Your default catalog: `labuser`\n",
    "    - Your configuration parameter: `source` = `/Volumes/dbacademy/ops/your-labuser-name`\n",
    "\n",
    "    **NOTE:** If the pipeline already exists, an error will be returned. In that case, you'll need to delete the existing pipeline and rerun this cell.\n",
    "\n",
    "    To delete the pipeline:\n",
    "\n",
    "    a. Select **Jobs & Pipelines** from the far-left navigation bar.  \n",
    "\n",
    "    b. Find the pipeline you want to delete.  \n",
    "\n",
    "    c. Click the three-dot menu ![ellipsis icon](./Includes/images/ellipsis_icon.png).  \n",
    "\n",
    "    d. Select **Delete**.\n",
    "\n",
    "**NOTE:**  The `create_declarative_pipeline` function is a custom function built for this course to create the sample pipeline using the Databricks REST API. This avoids manually creating the pipeline and referencing the pipeline assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6eb0d92-ff9a-4a49-8769-cb0d0d22c0e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "create_declarative_pipeline(pipeline_name=f'5 - Deploying a Pipeline to Production Project - {DA.catalog_name}', \n",
    "                            root_path_folder_name='5 - Deploying a Pipeline to Production Project',\n",
    "                            catalog_name = DA.catalog_name,\n",
    "                            schema_name = 'default',\n",
    "                            source_folder_names=['orders', 'status'],\n",
    "                            configuration = {'source':DA.paths.working_dir})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05e5a3f-7cc1-47f9-a593-e709b7a79fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Complete the following steps to open the starter Spark Declarative Pipeline project for this demonstration:\n",
    "\n",
    "   a. In the main navigation bar right-click on **Jobs & Pipelines** and select **Open in Link in New Tab**.\n",
    "\n",
    "   b. In **Jobs & Pipelines** select your **5 - Deploying a Pipeline to Production Project - labuser** pipeline.\n",
    "\n",
    "   c. **REQUIRED:** At the top near your pipeline name, turn on **New pipeline monitoring**.\n",
    "\n",
    "   d. In the **Pipeline details** pane on the far right, select **Open in Editor** (field to the right of **Source code**) to open the pipeline in the **Lakeflow Pipeline Editor**.\n",
    "\n",
    "   e. In the new tab you should see three folders: **explorations**, **orders**, and **status** (plus the extra **python_excluded** folder that contains the Python version). \n",
    "\n",
    "   f. Continue to step 4 and 5 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675ab57e-e6d3-414c-967f-fb1687c5afb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Explore the code in the `orders/orders_pipeline.sql` file\n",
    "\n",
    "4. In the new tab select the **orders** folder. It contains the same **orders_pipeline.sql** pipeline you've been working with.  **Follow the instructional comments in the file to proceed.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7280aaa2-c262-405f-a0cf-2f4e1614eb63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Explore the code in the `status/status_pipeline` notebook\n",
    "\n",
    "5. After reviewing the **orders_pipeline.sql** file, you'll be directed to explore the **status/status_pipeline.sql** notebook. This notebook processes new data and adds it to the pipeline. **Follow the instructions provided in the notebook's markdown cells.**\n",
    "\n",
    "    **NOTE:** The **status/status_pipeline.sql**  notebook will go through setting up the pipeline settings, scheduling and running the production pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e3c3dc7-0655-4a40-b388-1aa3d0a8c816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Land More Data to Your Data Source Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4975d9e1-d834-49fd-b903-21f9f38baa8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to add **4** more JSON files to your volumes:\n",
    "    - `/Volumes/dbacademy/ops/your-labuser-volume/orders`\n",
    "    - `/Volumes/dbacademy/ops/your-labuser-volume/status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f95eda6-5fb4-41ad-b2fb-8e936a0eeeae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "copy_files(copy_from = '/Volumes/dbacademy_retail/v01/retail-pipeline/orders/stream_json', \n",
    "           copy_to = f'{DA.paths.working_dir}/orders', \n",
    "           n = 5)\n",
    "\n",
    "copy_files(copy_from = '/Volumes/dbacademy_retail/v01/retail-pipeline/status/stream_json', \n",
    "           copy_to = f'{DA.paths.working_dir}/status', \n",
    "           n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22c8d03b-5022-4f0b-9608-25848c41668e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Navigate back to your pipeline and select **Run pipeline** to process the new landed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde66bea-aa4b-4682-a966-b1a5170793b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Introduction to the Pipeline Event Log (Advanced Topic)\n",
    "\n",
    "After running your pipeline and successfully publishing the event log as a table named **event_log_demo_5** in your **labuser.default** schema (database), begin exploring the event log. \n",
    "\n",
    "Here we will quickly introduce the event log. **To process the event log you will need knowledge of parsing JSON formatted strings.**\n",
    "\n",
    "  - [Monitor Lakeflow Spark Declarative Pipelines](https://docs.databricks.com/aws/en/dlt/observability) documentation\n",
    "\n",
    "**TROUBLESHOOT:** \n",
    "- **REQUIRED:** If you did not run the pipeline and publish the event log, the code below will not run. Please make sure to complete all steps before starting this section.\n",
    "\n",
    "- **HIDDEN EVENT LOG:** By default, Spark Declarative Pipelines writes the event log to a hidden Delta table in the default catalog and schema configured for the pipeline. While hidden, the table can still be queried by all sufficiently privileged users. By default, only the owner of the pipeline can query the event log table. By default, the name for the hidden event log is formatted as:  \n",
    "  - `catalog.schema.event_log_{pipeline_id}` - where the pipeline ID is the system-assigned UUID with dashes replaced by underscores.  \n",
    "  - [Query the Event Log](https://docs.databricks.com/aws/en/dlt/observability#query-the-event-log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b59c695-4a70-4e8f-9cd0-ad78875ebfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Complete the following steps to view the **labuser.default.event_log_demo_5** event log in your catalog:\n",
    "\n",
    "   a. Select the catalog icon ![Catalog Icon](./Includes/images/catalog_icon.png) from the left navigation pane.\n",
    "\n",
    "   b. Expand your **labuser** catalog.\n",
    "\n",
    "   c. Expand the following schemas (databases):\n",
    "      - **1_bronze_db**\n",
    "      - **2_silver_db**\n",
    "      - **3_gold_db**\n",
    "      - **default**\n",
    "\n",
    "   d. Notice the following:\n",
    "      - In the **1_bronze_db**, **2_silver_db**, and **3_gold_db** schemas, the pipeline streaming tables and materialized views were created (they end with **demo5**).\n",
    "      - In the **default** schema, the pipeline has published the event log as a table named **event_log_demo_5**.\n",
    "\n",
    "**NOTE:** You might need to refresh the catalogs to view the streaming tables, materialized views, and event log.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0a7c075-f265-46da-9d96-5bf937b56efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Query your **labuser.default.event_log_demo_5** table to see what the event log looks like.\n",
    "\n",
    "   Notice that it contains all events within the pipeline as **STRING** columns (typically JSON-formatted strings) or **STRUCT** columns. Databricks supports the `:` (colon) operator to parse JSON fields. See the [`:` operator documentation](https://docs.databricks.com/) for more details.\n",
    "\n",
    "   The following table describes the event log schema. Some fields contain JSON data—such as the **details** field—which must be parsed to perform certain queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba21e717-3236-4a31-8c5e-385462d1733c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM default.event_log_demo_5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd7219c6-ffa8-487d-9117-919e2c1de425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Field          | Description |\n",
    "|----------------|-------------|\n",
    "| `id`           | A unique identifier for the event log record. |\n",
    "| `sequence`     | A JSON document containing metadata to identify and order events. |\n",
    "| `origin`       | A JSON document containing metadata for the origin of the event, for example, the cloud provider, the cloud provider region, user_id, pipeline_id, or pipeline_type to show where the pipeline was created, either DBSQL or WORKSPACE. |\n",
    "| `timestamp`    | The time the event was recorded. |\n",
    "| `message`      | A human-readable message describing the event. |\n",
    "| `level`        | The event type, for example, INFO, WARN, ERROR, or METRICS. |\n",
    "| `maturity_level` | The stability of the event schema. The possible values are:<br><br>- **STABLE**: The schema is stable and will not change.<br>- **NULL**: The schema is stable and will not change. The value may be NULL if the record was created before the maturity_level field was added (release 2022.37).<br>- **EVOLVING**: The schema is not stable and may change.<br>- **DEPRECATED**: The schema is deprecated and the pipeline runtime may stop producing this event at any time. |\n",
    "| `error`        | If an error occurred, details describing the error. |\n",
    "| `details`      | A JSON document containing structured details of the event. This is the primary field used for analyzing events. |\n",
    "| `event_type`   | The event type. |\n",
    "\n",
    "**[Event Log Schema](https://docs.databricks.com/aws/en/ldp/monitor-event-log-schema)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdb83c76-b2fc-4c18-8d84-110a73c5b3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. The majority of the detailed information you will want from the event log is located in the **details** column, which is a JSON-formatted string. You will need to parse this column.\n",
    "\n",
    "   You can find more information in the Databricks documentation on how to [query JSON strings](https://docs.databricks.com/aws/en/semi-structured/json).\n",
    "\n",
    "   The code below will:\n",
    "\n",
    "   - Return the **event_type** column.\n",
    "\n",
    "   - Return the entire **details** JSON-formatted string.\n",
    "\n",
    "   - Parse out the **flow_progress** values from the **details** JSON-formatted string, if they exist.\n",
    "\n",
    "   - Parse out the **user_action** values from the **details** JSON-formatted string, if they exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2e7d487-95b7-472b-882f-66d1d685248d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  id,\n",
    "  event_type,\n",
    "  details,\n",
    "  details:flow_progress,\n",
    "  details:user_action\n",
    "FROM default.event_log_demo_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d260bd5a-daa5-4233-9c6c-845e239957de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. One use case for the event log is to examine data quality metrics for all runs of your pipeline. These metrics provide valuable insights into your pipeline, both in the short term and long term. Metrics are captured for each constraint throughout the entire lifetime of the table.\n",
    "\n",
    "   Below is an example query to obtain those metrics. We won’t dive into the JSON parsing code here. This example simply demonstrates what’s possible with the **event_log**.\n",
    "\n",
    "   Run the cell and observe the results. Notice the following:\n",
    "   - The **passing_records** for each constraint are displayed.\n",
    "   - The **failing_records** (WARN) for each constraint are displayed.\n",
    "\n",
    "**NOTE:** If you have selected **Run pipeline with full table refresh** at any time during your pipeline, your results will include metrics from previous runs as well as from the full refresh. Additional logic is required to isolate results after the full table refresh. This is outside the scope of this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1114268-89e1-43ac-a69a-ee06b522b4f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMPORARY VIEW dq_source_vw AS\n",
    "SELECT explode(\n",
    "            from_json(details:flow_progress:data_quality:expectations,\n",
    "                      \"array<struct<name: string, dataset: string, passed_records: int, failed_records: int>>\")\n",
    "          ) AS row_expectations\n",
    "   FROM default.event_log_demo_5\n",
    "   WHERE event_type = 'flow_progress';\n",
    "\n",
    "\n",
    "-- View the data\n",
    "SELECT \n",
    "  row_expectations.dataset as dataset,\n",
    "  row_expectations.name as expectation,\n",
    "  SUM(row_expectations.passed_records) as passing_records,\n",
    "  SUM(row_expectations.failed_records) as warnings_records\n",
    "FROM dq_source_vw\n",
    "GROUP BY row_expectations.dataset, row_expectations.name\n",
    "ORDER BY dataset;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86c1b6f-efa6-437d-900b-c8e31688661e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "This was a quick introduction to the pipeline **event_log**. With the **event_log**, you can investigate all aspects of your pipeline runs to explore the runs as well as create overall reports. Feel free to investigate the **event_log** further on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d22fcf-a56a-4f84-941f-fab2a17b039b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Lakeflow Spark Declarative Pipelines properties reference](https://docs.databricks.com/aws/en/dlt/properties#dlt-table-properties)\n",
    "\n",
    "- [Table properties and table options](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-tblproperties)\n",
    "\n",
    "- [Triggered vs. continuous pipeline mode](https://docs.databricks.com/aws/en/dlt/pipeline-mode)\n",
    "\n",
    "- [Development and production modes](https://docs.databricks.com/aws/en/dlt/updates#development-and-production-modes)\n",
    "\n",
    "- [Monitor Lakeflow Spark Declarative Pipelines](https://docs.databricks.com/aws/en/dlt/observability)\n",
    "\n",
    "- **Materialized views include built-in optimizations where applicable:**\n",
    "  - [Incremental refresh for materialized views](https://docs.databricks.com/aws/en/optimizations/incremental-refresh)\n",
    "  - [Delta Live Tables Announces New Capabilities and Performance Optimizations](https://www.databricks.com/blog/2022/06/29/delta-live-tables-announces-new-capabilities-and-performance-optimizations.html)\n",
    "  - [Cost-effective, incremental ETL with serverless compute for Delta Live Tables pipelines](https://www.databricks.com/blog/cost-effective-incremental-etl-serverless-compute-delta-live-tables-pipelines)\n",
    "\n",
    "- **Stateful joins:** For stateful joins in pipelines (i.e., joining incrementally as data is ingested), refer to the [Optimize stateful processing in Lakeflow Spark Declarative Pipelines with watermarks](https://docs.databricks.com/aws/en/dlt/stateful-processing) documentation. **Stateful joins are an advanced topic and outside the scope of this course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c43e4fd-b8d2-4bf2-99f0-5b74323d9713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "5 - Deploying a Pipeline to Production",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "SQL",
   "language": "sql",
   "name": "sql"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
