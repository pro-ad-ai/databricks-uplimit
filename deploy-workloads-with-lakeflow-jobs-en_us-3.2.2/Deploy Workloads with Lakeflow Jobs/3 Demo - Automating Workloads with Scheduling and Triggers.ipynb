{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98847667-2bc1-4423-9876-b6b7928638a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "decafa52-594b-42eb-b9ee-6de9b5e7271f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3 - Automating Workloads with Scheduling and Triggers\n",
    "\n",
    "In this lesson, we will explore the different scheduling options available for your jobs. We will also learn how to add a file arrival trigger to a task.\n",
    "\n",
    "This demo will cover:\n",
    "- Exploring the various scheduling options under Schedules & Triggers\n",
    "- Adding parameters to your job\n",
    "- Configuring a file arrival trigger for your job\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "- Automate your job\n",
    "- Use parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5958327-5845-40fd-a4f1-bc3dd37eea34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE (The cluster named 'labuser')\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "369816c1-c8c0-4002-998a-1576166584d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "\n",
    "\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course.\n",
    "\n",
    "**NOTE:** If you use Serverless V1 a warning will be returned. You can ignore the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d611170f-510b-4d56-ba97-a8f80e73484c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d7a7cd-5942-4fa6-831e-a89363fb4576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore Your Schema\n",
    "Complete the following to explore your **dbacademy.labuser** schema:\n",
    "\n",
    "1. In the left navigation bar, select the catalog icon:  ![Catalog Icon](./Includes/images/catalog_icon.png)\n",
    "\n",
    "2. Locate the catalog called **dbacademy** and expand the catalog.\n",
    "\n",
    "3. Expand your **labuser** schema. \n",
    "\n",
    "4. Notice that within your schema you will find two tables named as **sales_bronze** and **orders_bronze**.\n",
    "\n",
    "**Note:** If you have completed the 2L Exercise, you may find additional tables under your schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8236a8-a7ca-417b-b64f-5d60d86e0f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. View Your Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "270557c4-fe57-48d8-b7be-277154972429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Complete the following steps to view the notebook file you will use in this job. All files are located in the **Task Files** folder within the directory for the corresponding lesson number.\n",
    "\n",
    "1. Navigate to (or click the link for) the notebook: [Task Files/Lesson 3 Files/3.1 - Creating customers table]($./Task Files/Lesson 3 Files/3.1 - Creating customers table).  \n",
    "  - Review the notebook. Note that it stores multiple widget values (passed as parameters) in a variable and creates a new table called **customers_bronze**.\n",
    "\n",
    "2. After reviewing the notebook file, close it and return to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "421ce2f8-0412-44b0-9d59-7ff686a4f4f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Create the Job\n",
    "\n",
    "Complete the steps below to add new task into your Retail Job\n",
    "<!-- Run the cell below to automatically create the starter job for this demonstration (The starter job includes all tasks completed in **1 Demo - Creating a Job Using Lakeflow Jobs UI).**.\n",
    "\n",
    "\n",
    "    **NOTE:** The code below creates the required Databricks job for this demonstration using the [Databricks SDK](https://docs.databricks.com/aws/en/dev-tools/sdk-python). The method for creating the job is defined in the [Classroom-Setup-Common]($./Includes/Classroom-Setup-Common) notebook. While the [Databricks SDK](https://databricks-sdk-py.readthedocs.io/en/latest/) is used here, the SDK is beyond the scope of this course. Feel free to explore on your own. -->\n",
    "\n",
    "###D1. Creating the starter Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f140207-b650-4cea-995a-cd9363a09489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Creates the starter job from the end of '1 Demo - Creating a Job Using Lakeflow Jobs UI'\n",
    "\n",
    "job_tasks = [\n",
    "    {\n",
    "        'task_name': 'ingesting_orders',\n",
    "        'file_path': '/Task Files/Lesson 1 Files/1.1 - Creating orders table',\n",
    "        'depends_on': None\n",
    "    },\n",
    "    {\n",
    "        'task_name': 'ingesting_sales',\n",
    "        'file_path': '/Task Files/Lesson 1 Files/1.2 - Creating sales table',\n",
    "        'depends_on': None\n",
    "    }\n",
    "]\n",
    " \n",
    "myjob = DAJobConfig(job_name=f\"Demo_03_Retail_Job_{DA.schema_name}\",\n",
    "                        job_tasks=job_tasks,\n",
    "                        job_parameters=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "503be567-cdd9-4d9b-8962-80d2b00051d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Confirming your Job Details\n",
    "Complete the following steps to confirm that the new starter job, which begins with **Demo_03**, was created successfully and matches the job from **1 Demo - Creating a Job Using Lakeflow Jobs UI**:\n",
    "\n",
    "   a. Right-click on **Jobs and Pipelines** in the left navigation bar and select *Open Link in New Tab*.\n",
    "\n",
    "   b. Confirm that you see the job **Demo_03_Retail_Job_your-labuser-name**. Click the job to open it.\n",
    "\n",
    "   c. Select **Tasks** in the top navigation bar. The job should contain two tasks named **ingesting_orders** and **ingesting_sales**.\n",
    "\n",
    "   d. View the **Job details** section of the job. Confirm that **Performance optimized** mode enabled.\n",
    "\n",
    "   e. Note that this is the same job created in **1 Demo - Creating a Job Using Lakeflow Jobs UI**.\n",
    "\n",
    "   f. Leave the job page open and return to the instructions below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5b5ba2-9cc2-44fd-bade-a3dca3f14678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D3. Add a New Task to the Starter Job\n",
    "\n",
    "So far in our job we have ingested two tables into our job. \n",
    "\n",
    "Next, we will add a new table via notebook Task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "775fe52e-a2ec-478c-8682-b4ff05158a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Explore the notebook you will add to your job.\n",
    "\n",
    "   You can find the notebook in **Task Files** > **Lesson 3 Files** > **3.1 - Creating customers table**. \n",
    "\n",
    "   **NOTE:** Direct link to the notebook: [Task Files/Lesson 3 Files/3.1 - Creating customers table]($./Task Files/Lesson 3 Files/3.1 - Creating customers table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28515e17-5cbc-4395-ba54-9d9481efb461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "2. Complete the following steps to add the **3.1 - Creating customers table** notebook as a task in your job.\n",
    "\n",
    "   a. If you are not in your **Demo_03_Retail_Job_your-labuser-name** job, right-click the **Jobs and Pipelines** button in the sidebar and select *Open Link in New Tab*.\n",
    "\n",
    "   b. Find the **Demo_03_Retail_Job_your-labuser-name** job and go to the **Tasks** tab. \n",
    "\n",
    "   c. Click **Add task**, then select **Notebook**.\n",
    "\n",
    "   d. Configure the task as specified below and Click **Create task** to save the task:\n",
    "\n",
    "| Setting   | Instructions |\n",
    "|-----------|--------------|\n",
    "| **Task name** | Enter **ingesting_customers** |\n",
    "| **Type**      | Ensure **Notebook** is selected |\n",
    "| **Source**    | Ensure **Workspace** is selected |\n",
    "| **Path**      | Use the navigator to specify the path to [Task Files/Lesson 3 Files/3.1 - Creating customers table]($./Task Files/Lesson 3 Files/3.1 - Creating customers table)|\n",
    "| **Compute**   | From the dropdown menu, select a **Serverless** cluster. (We will be using Serverless clusters for jobs in this course. You can specify a different cluster if required outside of this course.) |\n",
    "| **Depends On**| None |\n",
    "\n",
    "\n",
    "![Lesson03_Notebook_task.png](./Includes/images/Lesson03_Notebook_task.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16fc05ee-9d3d-487e-b324-80cba9cff3f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Explore Scheduling Options\n",
    "\n",
    "Complete the following steps to explore scheduling options and triggers within Lakeflow Jobs.\n",
    "\n",
    "1. Return to your job.\n",
    "\n",
    "2. Make sure you are in the **Tasks** tab of your job.\n",
    "\n",
    "3. On the right-hand side of the Jobs UI, locate the **Job Details** section.  \n",
    "   - **NOTE:** If the side panel is collapsed, click the left-pointing arrow icon to expand it.\n",
    "\n",
    "4. Under the **Schedules & Triggers** section, click the **Add trigger** button to explore the options. There are three options (in addition to manual):\n",
    "\n",
    "   - **Scheduled** — you will see two types of schedule type **Simple** and **Advanced** \n",
    "      - Simple: This provides options for scheduling periodic job runs on a daily, hourly, or weekly basis.\n",
    "      - Advanced: This option allows you to schedule jobs using CRON syntax for precise timing.\n",
    "\n",
    "   - **Table Update** - A trigger can be set up in platforms like Azure Databricks to run automatically whenever one or more specified tables are updated.\n",
    "\n",
    "   - **Continuous** — runs repeatedly with a short interval between runs.\n",
    "\n",
    "   - **File arrival** — monitors an external location or volume for new files. Note the **Advanced** settings, where you can adjust the time between checks and the delay after a new file arrives before starting a run.\n",
    "\n",
    "5. Leave the **Schedules & Triggers** panel open and return to the instructions below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0cc090e-a78f-47a0-883d-58c5dc2cd968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Configure the File Arrival Trigger for the Job\n",
    "\n",
    "In this step, we will set up a file arrival trigger to monitor a designated volume for new data files. The objective is to automatically start the job whenever a new file is detected in the specified location, enabling seamless and timely data processing.\n",
    "\n",
    "\n",
    "  **NOTE:**  Databricks volumes are Unity Catalog objects representing a logical volume of storage in a cloud object storage location. Volumes provide capabilities for accessing, storing, governing, and organizing files. You can use volumes to store and access files in any format, including structured, semi-structured, and unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b809f7e-7998-4de4-9eef-2c26860c687c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to create a volume named **trigger_storage_location**. This volume will serve as the storage location to monitor for new files.  \n",
    "\n",
    "   It will be created within the **dbacademy** catalog, inside your unique **labuser** schema.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ed7aa7-2a7f-4f4d-90c9-ad8e99b8f36a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS trigger_storage_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5d44243-5f8f-4e3c-92bf-991d739e0006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the following steps to view your new volume **trigger_storage_location** in your **dbacademy.labuser** schema:\n",
    "\n",
    "   a. In the left navigation bar, click the **Catalog** ![Catalog Icon](./Includes/images/catalog_icon.png) icon.\n",
    "\n",
    "   b. Locate and expand the catalog named **dbacademy**.\n",
    "\n",
    "   c. Expand your **labuser** schema.\n",
    "\n",
    "   d. Expand **Volumes** and confirm that the **trigger_storage_location** volume appears.\n",
    "\n",
    "   e. Expand the **trigger_storage_location** volume and verify that it does **not** contain any files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4d2962f-75b9-415c-bec1-642547668eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. You can also use the `SHOW VOLUMES` statement to view available volumes in your schema (database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f4b9755-35aa-4eee-9939-53923c623426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW VOLUMES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e1265f7-9392-4aa8-b75e-8426642ae126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Run the following cell to get the path to this volume using the custom `DA` object created for this course.\n",
    "\n",
    "    **NOTE:** You can also select your volume under the catalog, click the three ellipses, and then select *Copy volume path* to get the volume path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35954ffa-2409-438c-8504-d4315a920ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "your_volume_path = (f\"/Volumes/{DA.catalog_name}/{DA.schema_name}/trigger_storage_location/\")\n",
    "print(your_volume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d87870-d757-411f-8df6-5b1c459ec635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Complete the following to configure the **File Arrival** trigger on your job:\n",
    "\n",
    "   a. Navigate back to the browser tab with your job.\n",
    "\n",
    "   b. In your job, **click Add Trigger** in the Job details pane, and under Trigger type, select File Arrival for the trigger type.\n",
    "\n",
    "   c. Paste the path above into the **Storage location** field\n",
    "\n",
    "   d. Click **Test Trigger** to verify the correct path\n",
    "\n",
    "    - **NOTE:** You should see **Success**. If not, verify that you have run the cell above and copied all of the cell output into **Storage location**\n",
    "\n",
    "   e. Expand the **Advanced** options. Notice that you can set different trigger options.\n",
    "\n",
    "   f. Click **Save**\n",
    "\n",
    "**NOTE:**  There is a limit of 1000 files that can be triggered using file arrival trigger. \n",
    "\n",
    "For Reference: https://docs.databricks.com/aws/en/jobs/file-arrival-triggers#limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c5bbc6-dfdd-41aa-88aa-fddb45ce3d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## G. Setting Task Parameters\n",
    "The Task notebook for this demo will needs to know the name of the catalog and schema we are working with. We can configure this using **Task parameters** (you can also use **Job Parameters** and it will get pushed down to all tasks). \n",
    "\n",
    "  This provides flexibility and the ability to reuse code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "550a92ff-9486-4094-9b09-783ba15504cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to view your **catalog** and **schema** names. We will need this when setting our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfef60ac-cfcc-4e60-aab1-e0a93c747f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"catalog : {DA.catalog_name}\")\n",
    "print(f\"schema : {DA.schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ebf38ff-66ce-405b-b7c7-4ea27188f030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the following steps to **set the task parameters**:\n",
    "\n",
    "   a. Go back to your Task **ingesting_customers**. In **Task details** pane, under **Parameters**, click **Add**.\n",
    "\n",
    "   b. Set the following key-value pairs:\n",
    "   - **catalog** (key) =  **dbacademy** (value) \n",
    "   - **schema** (key) = your **labuser** schema name from the above cell output (value) \n",
    "\n",
    "   c. Click **Save task**.\n",
    "\n",
    "3.  Click to open the [Task Files/Lesson 3 Files/3.1 - Creating customers table]($./Task Files/Lesson 3 Files/3.1 - Creating customers table) notebook. This notebook is used in the **ingesting_customers** task. Notice the following in the notebook:\n",
    "\n",
    "      - The `my_catalog` variable is obtaining the value from the `catalog` parameter we set in the task using the following:\n",
    "         - `my_catalog = dbutils.widgets.get('catalog')`\n",
    "\n",
    "      - The `my_schema` variable is obtaining the value from the `schema` parameter we set in the task using the following: \n",
    "         - `my_schema = dbutils.widgets.get('schema')`\n",
    "\n",
    "      - The `my_volume_path` variable uses the parameters we set to point to your **trigger_storage_location** volume using:\n",
    "         - `f\"/Volumes/{my_catalog}/{my_schema}/trigger_storage_location/\"`\n",
    "\n",
    "4.  Close the **Creating customer table** notebook\n",
    "\n",
    "\n",
    "![Lesson03_TriggerJob.png](./Includes/images/Lesson03_TriggerJob.png)\n",
    "\n",
    "\n",
    "   Your File Arrival Trigger and ingesting_customers Task should look like the above screenshot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f5c99cf-08be-492f-9d62-c19bb3f473a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##H. Adding a File into Volume\n",
    "Run the cell below to land a new file in your volume **trigger_storage_location**. \n",
    "\n",
    "For the File Arrival Trigger, only new files are ingested and processed. Modified files will not trigger the run again. To re-trigger, you need to manually update the file name.\n",
    "\n",
    "**NOTE:** This is a custom course function we created for adding data to your volume to mimic data being loaded to cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3ff0095-96c8-496c-b5b4-5e1e11d6e765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.copy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e08f03-9772-4a95-b484-60346156092a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## I. Monitoring the Run\n",
    "\n",
    "Once the trigger is configured, Databricks will automatically monitor the storage location for new files (checked every minute by default). Follow these steps to monitor your job runs:\n",
    "\n",
    "1. **Open the Runs Tab**\n",
    "   - In the upper-left corner, click the **Runs** tab.\n",
    "   - Look for the **Trigger status**. If you don't see it, wait a minute and verify your **File arrival** trigger setup if needed.\n",
    "\n",
    "2. **Check Trigger Evaluation**\n",
    "   - The trigger will show as evaluated. If no new files are found, the job will not run.\n",
    "\n",
    "3. **View Job Run Details**\n",
    "   - After the job runs (usually within 1-2 minutes), click the **Start time** to see run details.\n",
    "\n",
    "> **Tip:**  \n",
    "> To manually trigger a run with different parameters, go to the job configuration page, click **Edit task** from the **Run output** page, then click the down arrow next to **Run now** and select **Run now with different settings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4f33a5-8b60-4b0e-bcd5-d151bca0132d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## J. Viewing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "215c6c92-aa35-4ca4-8ca5-16937ca2d8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab3c280-5ee8-461c-81ae-2a194d2bf705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM customers_bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa35500f-c2ea-4099-9ae4-a269fbcfc7ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Additional Resources\n",
    "[Parameterize jobs](https://docs.databricks.com/aws/en/jobs/parameters) documentation\n",
    "\n",
    "[Automating jobs with schedules and triggers](https://docs.databricks.com/aws/en/jobs/triggers) documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "872e6e49-de96-4d6a-89b3-0922c6d7d7bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "3 Demo - Automating Workloads with Scheduling and Triggers",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
